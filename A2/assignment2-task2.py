# -*- coding: utf-8 -*-
"""A2-Task2.ipynb”

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xBwMxcrUVH7Uyre4pKd_fRjFb21re9X

<div style="text-align: right">   </div>


Introduction to Deep Learning (2024) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;
-------|-------------------
**Assignment 2 - Sequence processing using RNNs** | <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/UniversiteitLeidenLogo.svg/1280px-UniversiteitLeidenLogo.svg.png" width="300">



# Introduction


The goal of this assignment is to learn how to use encoder-decoder recurrent neural networks (RNNs). Specifically we will be dealing with a sequence to sequence problem and try to build recurrent models that can learn the principles behind simple arithmetic operations (**integer addition, subtraction and multiplication.**).

<img src="https://i.ibb.co/5Ky5pbk/Screenshot-2023-11-10-at-07-51-21.png" alt="Screenshot-2023-11-10-at-07-51-21" border="0" width="500"></a>

In this assignment you will be working with three different kinds of models, based on input/output data modalities:
1. **Text-to-text**: given a text query containing two integers and an operand between them (+ or -) the model's output should be a sequence of integers that match the actual arithmetic result of this operation
2. **Image-to-text**: same as above, except the query is specified as a sequence of images containing individual digits and an operand.
3. **Text-to-image**: the query is specified in text format as in the text-to-text model, however the model's output should be a sequence of images corresponding to the correct result.


### Description**
Let us suppose that we want to develop a neural network that learns how to add or subtract
two integers that are at most two digits long. For example, given input strings of 5 characters: ‘81+24’ or
’41-89’ that consist of 2 two-digit long integers and an operand between them, the network should return a
sequence of 3 characters: ‘105 ’ or ’-48 ’ that represent the result of their respective queries. Additionally,
we want to build a model that generalizes well - if the network can extract the underlying principles behind
the ’+’ and ’-’ operands and associated operations, it should not need too many training examples to generate
valid answers to unseen queries. To represent such queries we need 13 unique characters: 10 for digits (0-9),
2 for the ’+’ and ’-’ operands and one for whitespaces ’ ’ used as padding.
The example above describes a text-to-text sequence mapping scenario. However, we can also use different
modalities of data to represent our queries or answers. For that purpose, the MNIST handwritten digit
dataset is going to be used again, however in a slightly different format. The functions below will be used to create our datasets.

---

*To work on this notebook you should create a copy of it.*

# Function definitions for creating the datasets

First we need to create our datasets that are going to be used for training our models.

In order to create image queries of simple arithmetic operations such as '15+13' or '42-10' we need to create images of '+' and '-' signs using ***open-cv*** library. We will use these operand signs together with the MNIST dataset to represent the digits.
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import cv2
import numpy as np
import tensorflow as tf
import random
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense, RNN, LSTM, Flatten, TimeDistributed, LSTMCell
from tensorflow.keras.layers import RepeatVector, Conv2D, SimpleRNN, GRU, Reshape, ConvLSTM2D,MaxPooling2D
from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Conv2DTranspose, Reshape, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError

from scipy.ndimage import rotate
# Create plus/minus operand signs
def generate_images(number_of_images=50, sign='-'):
    blank_images = np.zeros([number_of_images, 28, 28])  # Dimensionality matches the size of MNIST images (28x28)
    x = np.random.randint(12, 16, (number_of_images, 2)) # Randomized x coordinates
    y1 = np.random.randint(6, 10, number_of_images)       # Randomized y coordinates
    y2 = np.random.randint(18, 22, number_of_images)     # -||-

    for i in range(number_of_images): # Generate n different images
        cv2.line(blank_images[i], (y1[i], x[i,0]), (y2[i], x[i, 1]), (255,0,0), 2, cv2.LINE_AA)     # Draw lines with randomized coordinates
        if sign == '+':
            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA) # Draw lines with randomized coordinates
        if sign == '*':
            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA)
            # Rotate 45 degrees
            blank_images[i] = rotate(blank_images[i], -50, reshape=False)
            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA)
            blank_images[i] = rotate(blank_images[i], -50, reshape=False)
            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA)

    return blank_images

def show_generated(images, n=5):
    plt.figure(figsize=(2, 2))
    for i in range(n**2):
        plt.subplot(n, n, i+1)
        plt.axis('off')
        plt.imshow(images[i])
    plt.show()

show_generated(generate_images())
show_generated(generate_images(sign='+'))

def create_data(highest_integer, num_addends=2, operands=['+', '-']):
    """
    Creates the following data for all pairs of integers up to [1:highest integer][+/-][1:highest_integer]:

    @return:
    X_text: '51+21' -> text query of an arithmetic operation (5)
    X_img : Stack of MNIST images corresponding to the query (5 x 28 x 28) -> sequence of 5 images of size 28x28
    y_text: '72' -> answer of the arithmetic text query
    y_img :  Stack of MNIST images corresponding to the answer (3 x 28 x 28)

    Images for digits are picked randomly from the whole MNIST dataset.
    """
    num_indices = [np.where(MNIST_labels==x) for x in range(10)]
    num_data = [MNIST_data[inds] for inds in num_indices]
    image_mapping = dict(zip(unique_characters[:10], num_data))
    image_mapping['-'] = generate_images()
    image_mapping['+'] = generate_images(sign='+')
    image_mapping['*'] = generate_images(sign='*')
    image_mapping[' '] = np.zeros([1, 28, 28])

    X_text, X_img, y_text, y_img = [], [], [], []

    for i in range(highest_integer + 1):      # First addend
        for j in range(highest_integer + 1):  # Second addend
            for sign in operands: # Create all possible combinations of operands
                query_string = to_padded_chars(str(i) + sign + str(j), max_len=max_query_length, pad_right=True)
                query_image = []
                for n, char in enumerate(query_string):
                    image_set = image_mapping[char]
                    index = np.random.randint(0, len(image_set), 1)
                    query_image.append(image_set[index].squeeze())

                result = eval(query_string)
                result_string = to_padded_chars(result, max_len=max_answer_length, pad_right=True)
                result_image = []
                for n, char in enumerate(result_string):
                    image_set = image_mapping[char]
                    index = np.random.randint(0, len(image_set), 1)
                    result_image.append(image_set[index].squeeze())

                X_text.append(query_string)
                X_img.append(np.stack(query_image))
                y_text.append(result_string)
                y_img.append(np.stack(result_image))

    return np.stack(X_text), np.stack(X_img)/255., np.stack(y_text), np.stack(y_img)/255.

def to_padded_chars(integer, max_len=3, pad_right=False):
    """
    Returns a string of len()=max_len, containing the integer padded with ' ' on either right or left side
    """
    length = len(str(integer))
    padding = (max_len - length) * ' '
    if pad_right:
        return str(integer) + padding
    else:
        return padding + str(integer)

"""# Creating our data

The dataset consists of 20000 samples that (additions and subtractions between all 2-digit integers) and they have two kinds of inputs and label modalities:

  **X_text**: strings containing queries of length 5: ['  1+1  ', '11-18', ...]

  **X_image**: a stack of images representing a single query, dimensions: [5, 28, 28]

  **y_text**: strings containing answers of length 3: ['  2', '156']

  **y_image**: a stack of images that represents the answer to a query, dimensions: [3, 28, 28]
"""

# Illustrate the generated query/answer pairs
unique_characters = '0123456789+- '       # All unique characters that are used in the queries (13 in total: digits 0-9, 2 operands [+, -], and a space character ' '.)
highest_integer = 99                      # Highest value of integers contained in the queries

max_int_length = len(str(highest_integer))# Maximum number of characters in an integer
max_query_length = max_int_length * 2 + 1 # Maximum length of the query string (consists of two integers and an operand [e.g. '22+10'])
max_answer_length = 3    # Maximum length of the answer string (the longest resulting query string is ' 1-99'='-98')

# Create the data (might take around a minute)
(MNIST_data, MNIST_labels), _ = tf.keras.datasets.mnist.load_data()
X_text, X_img, y_text, y_img = create_data(highest_integer)
print(X_text.shape, X_img.shape, y_text.shape, y_img.shape)
## Display the samples that were created
def display_sample(n):
    labels = ['X_img:', 'y_img:']
    for i, data in enumerate([X_img, y_img]):
        plt.subplot(1,2,i+1)
        # plt.set_figheight(15)
        plt.axis('off')
        plt.title(labels[i])
        plt.imshow(np.hstack(data[n]), cmap='gray')
    print('='*50, f'\nQuery #{n}\n\nX_text: "{X_text[n]}" = y_text: "{y_text[n]}"')
    plt.show()

for _ in range(10):
    display_sample(np.random.randint(0, 10000, 1)[0])
    print()

"""## Helper functions

The functions below will help with input/output of the data.
"""

# One-hot encoding/decoding the text queries/answers so that they can be processed using RNNs
# You should use these functions to convert your strings and read out the output of your networks

def encode_labels(labels, max_len=3):
  n = len(labels)
  length = len(labels[0])
  char_map = dict(zip(unique_characters, range(len(unique_characters))))
  one_hot = np.zeros([n, length, len(unique_characters)])
  for i, label in enumerate(labels):
      m = np.zeros([length, len(unique_characters)])
      for j, char in enumerate(label):
          m[j, char_map[char]] = 1
      one_hot[i] = m

  return one_hot


def decode_labels(labels):
    pred = np.argmax(labels, axis=1)
    predicted = ''.join([unique_characters[i] for i in pred])

    return predicted

X_text_onehot = encode_labels(X_text)
y_text_onehot = encode_labels(y_text)

print(X_text_onehot.shape, y_text_onehot.shape)

"""---
---

## I. Text-to-text RNN model

The following code showcases how Recurrent Neural Networks (RNNs) are built using Keras. Several new layers are going to be used:

1. LSTM
2. TimeDistributed
3. RepeatVector

The code cell below explains each of these new components.

<img src="https://i.ibb.co/NY7FFTc/Screenshot-2023-11-10-at-09-27-25.png" alt="Screenshot-2023-11-10-at-09-27-25" border="0" width="500"></a>

"""

def build_text2text_model():
    # We start by initializing a sequential model
    text2text = tf.keras.Sequential()

    # "Encode" the input sequence using an RNN, producing an output of size 256.
    # In this case the size of our input vectors is [5, 13] as we have queries of length 5 and 13 unique characters. Each of these 5 elements in the query will be fed to the network one by one,
    # as shown in the image above (except with 5 elements).
    # Hint: In other applications, where your input sequences have a variable length (e.g. sentences), you would use input_shape=(None, unique_characters).
    text2text.add(LSTM(256, input_shape=(None, len(unique_characters))))
    # As the decoder RNN's input, repeatedly provide with the last output of RNN for each time step. Repeat 3 times as that's the maximum length of the output (e.g. '  1-99' = '-98')
    # when using 2-digit integers in queries. In other words, the RNN will always produce 3 characters as its output.
    text2text.add(RepeatVector(max_answer_length))
    # By setting return_sequences to True, return not only the last output but all the outputs so far in the form of (num_samples, timesteps, output_dim). This is necessary as TimeDistributed in the below expects
    # the first dimension to be the timesteps.
    text2text.add(LSTM(256, return_sequences=True))

    # Apply a dense layer to the every temporal slice of an input. For each of step of the output sequence, decide which character should be chosen.
    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))
    # Next we compile the model using categorical crossentropy as our loss function.
    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    text2text.summary()
    return text2text

"""# This is a structure of the text-to-text
It encodes the input sequence into a context vector and then decodes it to generate the output sequence.
"""

from tensorflow.keras.utils import plot_model
model = build_text2text_model()
plot_model(model, to_file='image2text_model.png', show_shapes=True, show_layer_names=True, dpi=72)

"""# Different test set in text-text
This dataset is split into training and testing sets using different test sizes, defined by the split_number array, which includes various values such as 0.1, 0.25, 0.4, and so on. For each of these test sizes, the model is trained on the training data (x_train and y_train) using 20 epochs and a batch size of 32. After each training run, the model is evaluated on the test data (x_test and y_test), and both the test loss and test accuracy are recorded
"""

from sklearn.model_selection import train_test_split
X_text_onehot = encode_labels(X_text, max_len=5)
y_text_onehot = encode_labels(y_text, max_len=3)
split_number=[0.1,0.25,0.4,0.5,0.6,0.75,0.9]
all_histories = {}
for i in split_number:
  x_train, x_test, y_train, y_test = train_test_split(X_text_onehot, y_text_onehot, test_size=i)
  model = build_text2text_model()
  history = model.fit(x_train, y_train, epochs=20, batch_size=32)
  test_loss, test_accuracy = model.evaluate(x_test, y_test)
  all_histories[i] = {
    'loss': history.history['loss'],
    'accuracy': history.history['accuracy'],
    'test_loss': test_loss,
    'test_accuracy': test_accuracy
}

fig, axes = plt.subplots(1, 2, figsize=(8, 5))
for i, history in all_histories.items():
  axes[0].plot(history['loss'], label=f'Training Loss (test_size={i})')
axes[0].set_title('Training Loss of Different Test Sizes')
axes[0].set_xlabel('Epochs')
axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid()
for i, history in all_histories.items():
  axes[1].plot(history['accuracy'], label=f'Training Accuracy (test_size={i})')
axes[1].set_title('Training Accuracy of Different Test Sizes')
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('Accuracy')
axes[1].legend()
axes[1].grid()
plt.tight_layout()
plt.show()
for split, metrics in all_histories.items():
    print(f"Test size: {split}")
    print(f"Test Loss: {metrics['test_loss']:.4f}, Test Accuracy: {metrics['test_accuracy']:.4f}")
    print("-" * 30)

"""# Comparison the errors with test-size=0.3
### Description**
This code evaluates the performance of a text-to-text model by testing it with various test set and analyzing its errors.

*   Function:This function identifies misclassified samples by comparing the model's predictions (y_pred) with the true labels (y_test).It decodes the one-hot encoded outputs into human-readable text using the provided unique_characters list.It prints the total number of misclassified samples and displays the first 10 examples, including the input query, true result, and predicted result.

*   Testing with different split ratios:
    

1.   The code uses a range of test set sizes (split_number), like 10%, 25%, etc., to divide the dataset into training and test sets.
2.   For each split ratio, The dataset is split into training and testing subsets using train_test_split.A new model is built and trained using build_text2text_model and the training data.The trained model is tested, and its prediction errors are analyzed using the analyze_errors function.





"""

import numpy as np
from collections import defaultdict
from sklearn.model_selection import train_test_split

def analyze_errors_and_length(model, x_test, y_test, x_test_text, y_test_text, unique_characters):
    def decode_output(output):
        return ''.join([unique_characters[np.argmax(i)] for i in output])
    y_pred = model.predict(x_test)
    misclassified_samples = [
        (x_test_text[idx], decode_output(y_test[idx]), decode_output(y_pred[idx]))
        for idx in range(len(x_test))
        if decode_output(y_test[idx]) != decode_output(y_pred[idx])
    ]
    print(f"Total Misclassified Samples: {len(misclassified_samples)}\n")
    error_by_length = defaultdict(int)
    length_count = defaultdict(int)
    large_errors = 0
    small_errors = 0
    unknown_errors = 0
    for query, true_label, predicted_label in misclassified_samples:
        query_no_spaces = query.replace(" ", "")
        query_length = len(query_no_spaces)
        error_by_length[query_length] += 1
        length_count[query_length] += 1
        try:
            true_value = float(true_label)
            predicted_value = float(predicted_label)
            value_diff = abs(true_value - predicted_value)
            if value_diff > 10:
                large_errors += 1
            elif 1 <= value_diff <= 9:
                small_errors += 1
            else:
                unknown_errors += 1
        except ValueError:
            unknown_errors += 1
        if len(misclassified_samples) <= 10:
            print(f"Query: {query} -> True Result: {true_label} -> Predicted Result: {predicted_label}")
    print("\nLength-based Misclassification Analysis:")
    lengths = sorted(error_by_length.keys())
    error_counts = [error_by_length[length] for length in lengths]
    total_counts = [length_count[length] for length in lengths]

    for i, length in enumerate(lengths):
        print(f"Length {length}: Total samples = {total_counts[i]}, Misclassified = {error_counts[i]}")
    print("\nError Type-based Misclassification Analysis:")
    print(f"Large Errors (>10 difference): {large_errors}")
    print(f"Small Errors (1-9 difference): {small_errors}")
    print(f"Unknown Errors (non-numeric or 0 difference): {unknown_errors}")

test_size = 0.3
x_train, x_test, y_train, y_test, x_train_text, x_test_text, y_train_text, y_test_text = train_test_split(
    X_text_onehot, y_text_onehot, X_text, y_text, test_size=test_size
)
model = build_text2text_model()
model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1)
analyze_errors_and_length(model, x_test, y_test, x_test_text, y_test_text, unique_characters)

"""# Add additional LSTM layer to encoder networks
This code compares the performance (accuracy and loss) of models with different numbers of LSTM layers. The key steps involve training models with 1, 2, 3, and 4 LSTM layers, and then visualizing the results (training accuracy and loss) for each configuration.

By train models with different numbers of LSTM layers, We can observe how the number of layers affects the training accuracy and loss.



*   Model Construction:

The build_text2text_model function constructs a model with a dynamic number of LSTM layers, controlled by the num_encoder_layers parameter. This function uses the Sequential API in TensorFlow to stack multiple LSTM layers.

The first LSTM layer has return_sequences=True to pass sequences to subsequent LSTM layers.

*  Training and Evaluation:

For each configuration of LSTM layers (1 to 4), the model is trained with data split into training and test sets.

The fit function tracks the training accuracy and loss for each epoch.

"""

all_histories = {}
num_encoder_layers_list = [1, 2, 3, 4]
for num_encoder_layers in num_encoder_layers_list:
  def build_text2text_model(num_encoder_layers):
    text2text = tf.keras.Sequential()
    text2text.add(LSTM(256, input_shape=(None, len(unique_characters)), return_sequences=True))
    for _ in range(num_encoder_layers - 1):
        text2text.add(LSTM(256, return_sequences=True))
    text2text.add(LSTM(256, return_sequences=False))
    text2text.add(RepeatVector(max_answer_length))
    text2text.add(LSTM(256, return_sequences=True))
    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))
    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    text2text.summary()
    return text2text
  X_text_onehot = encode_labels(X_text, max_len=5)
  y_text_onehot = encode_labels(y_text, max_len=3)
  x_train, x_test, y_train, y_test = train_test_split(X_text_onehot, y_text_onehot, test_size=0.1)
  model = build_text2text_model(num_encoder_layers=num_encoder_layers)
  history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.1)
  test_loss, test_accuracy = model.evaluate(x_test, y_test)
  all_histories[num_encoder_layers] = {
      'loss': history.history['loss'],
      'accuracy': history.history['accuracy'],
      'test_loss': test_loss,
      'test_accuracy': test_accuracy
  }

all_histories = {}
num_encoder_layers = 2
def build_text2text_model(num_encoder_layers):
    text2text = tf.keras.Sequential()
    text2text.add(LSTM(256, input_shape=(None, len(unique_characters)), return_sequences=True))
    # Add extra 2 LSTM layers based on num_encoder_layers
    for _ in range(num_encoder_layers - 1):
      text2text.add(LSTM(256, return_sequences=True))
    text2text.add(LSTM(256, return_sequences=False))
    text2text.add(RepeatVector(max_answer_length))
    text2text.add(LSTM(256, return_sequences=True))
    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))
    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    text2text.summary()
    return text2text
X_text_onehot = encode_labels(X_text, max_len=5)
y_text_onehot = encode_labels(y_text, max_len=3)
x_train, x_test, y_train, y_test = train_test_split(X_text_onehot, y_text_onehot, test_size=0.1)
model = build_text2text_model(num_encoder_layers=num_encoder_layers)
history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.1)
test_loss, test_accuracy = model.evaluate(x_test, y_test)
all_histories[num_encoder_layers] = {
    'loss': history.history['loss'],
    'accuracy': history.history['accuracy'],
    'test_loss': test_loss,
    'test_accuracy': test_accuracy
}

fig, axs = plt.subplots(1, 2, figsize=(10, 6))
for num_layers, history in all_histories.items():
    epochs = range(1, len(history['accuracy']) + 1)
    axs[0].plot(epochs, history['accuracy'], label=f'{num_layers} LSTM layers')
axs[0].set_title('Training Accuracy for Different LSTM Layer Numbers')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Accuracy')
axs[0].legend()
axs[0].grid()
for num_layers, history in all_histories.items():
    epochs = range(1, len(history['loss']) + 1)
    axs[1].plot(epochs, history['loss'], label=f'{num_layers} LSTM layers')

axs[1].set_title('Training Loss for Different LSTM Layer Numbers')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Loss')
axs[1].legend()
axs[1].grid()

plt.tight_layout()
plt.show()

"""# Errors in Different layers
In this section, we add LTSM layers in encoder process and record their performance. We used two dimensions to measure errors. The First one is errors types.



1.  The first method is of the wrong type. We define three types based on the  expected value and the actual value by calculating the difference between them. If the difference is greater than 10, it is considered a large error; if the difference is between 1 and 9, it is considered a small error; and if it cannot be recognized, it is classified as unknown or uncertain. We then calculate the number of occurrences for each type.

2. The second method calculates the error rate for each length based on the length of the input data.

We recorded the data for 1, 2, and 3 layers, and found that after 3 layers, the loss decreased more slowly, the error rate increased, and the performance declined.


"""

#2 LTSM layers
def build_text2text_model():
    text2text = tf.keras.Sequential()
    text2text.add(LSTM(256, input_shape=(None, len(unique_characters)), return_sequences=True))
    text2text.add(LSTM(256, return_sequences=True))
    # Second additional LSTM layer
    text2text.add(LSTM(256, return_sequences=True))
    text2text.add(LSTM(256, return_sequences=False))
    text2text.add(RepeatVector(max_answer_length))
    text2text.add(LSTM(256, return_sequences=True))
    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))
    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    text2text.summary()
    return text2text

import numpy as np
from collections import defaultdict
from sklearn.model_selection import train_test_split

def analyze_errors_and_length(model, x_test, y_test, x_test_text, y_test_text, unique_characters):
    def decode_output(output):
        return ''.join([unique_characters[np.argmax(i)] for i in output])

    y_pred = model.predict(x_test)
    misclassified_samples = [
        (x_test_text[idx], decode_output(y_test[idx]), decode_output(y_pred[idx]))
        for idx in range(len(x_test))
        if decode_output(y_test[idx]) != decode_output(y_pred[idx])
    ]
    print(f"Total Misclassified Samples: {len(misclassified_samples)}\n")

    error_by_length = defaultdict(int)
    length_count = defaultdict(int)
    large_errors = 0
    small_errors = 0
    unknown_errors = 0

    for query, true_label, predicted_label in misclassified_samples:
        query_no_spaces = query.replace(" ", "")
        query_length = len(query_no_spaces)
        error_by_length[query_length] += 1
        length_count[query_length] += 1

        try:
            true_value = float(true_label)
            predicted_value = float(predicted_label)
            value_diff = abs(true_value - predicted_value)
            if value_diff > 10:
                large_errors += 1
            elif 1 <= value_diff <= 9:
                small_errors += 1
            else:
                unknown_errors += 1
        except ValueError:
            unknown_errors += 1

        if len(misclassified_samples) <= 10:
            print(f"Query: {query} -> True Result: {true_label} -> Predicted Result: {predicted_label}")

    print("\nLength-based Misclassification Analysis:")
    lengths = sorted(error_by_length.keys())
    error_counts = [error_by_length[length] for length in lengths]
    total_counts = [length_count[length] for length in lengths]

    print("\nTotal counts and misclassification counts by query length (without spaces):")
    for length in lengths:
        total = length_count[length]
        misclassified = error_by_length[length]
        print(f"Length {length}: Total samples = {total}, Misclassified = {misclassified}")

    print("\nError Type-based Misclassification Analysis:")
    print(f"Large Errors (>10 difference): {large_errors}")
    print(f"Small Errors (1-9 difference): {small_errors}")
    print(f"Unknown Errors (non-numeric or 0 difference): {unknown_errors}")

test_size = 0.3
x_train, x_test, y_train, y_test, x_train_text, x_test_text, y_train_text, y_test_text = train_test_split(
    X_text_onehot, y_text_onehot, X_text, y_text, test_size=test_size
)

model = build_text2text_model()
model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1)

analyze_errors_and_length(model, x_test, y_test, x_test_text, y_test_text, unique_characters)

#3 LTSM layers
def build_text2text_model():
    text2text = tf.keras.Sequential()
    text2text.add(LSTM(256, input_shape=(None, len(unique_characters)), return_sequences=True))
    text2text.add(LSTM(256, return_sequences=True))
    text2text.add(LSTM(256, return_sequences=False))
    text2text.add(RepeatVector(max_answer_length))
    text2text.add(LSTM(256, return_sequences=True))
    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))
    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    text2text.summary()
    return text2text

import numpy as np
from collections import defaultdict
from sklearn.model_selection import train_test_split

def analyze_errors_and_length(model, x_test, y_test, x_test_text, y_test_text, unique_characters):
    def decode_output(output):
        return ''.join([unique_characters[np.argmax(i)] for i in output])

    y_pred = model.predict(x_test)
    misclassified_samples = [
        (x_test_text[idx], decode_output(y_test[idx]), decode_output(y_pred[idx]))
        for idx in range(len(x_test))
        if decode_output(y_test[idx]) != decode_output(y_pred[idx])
    ]

    print(f"Total Misclassified Samples: {len(misclassified_samples)}\n")

    error_by_length = defaultdict(int)
    length_count = defaultdict(int)
    large_errors = 0
    small_errors = 0
    unknown_errors = 0

    for query, true_label, predicted_label in misclassified_samples:
        query_no_spaces = query.replace(" ", "")
        query_length = len(query_no_spaces)
        error_by_length[query_length] += 1
        length_count[query_length] += 1

        try:
            true_value = float(true_label)
            predicted_value = float(predicted_label)
            value_diff = abs(true_value - predicted_value)
            if value_diff > 10:
                large_errors += 1
            elif 1 <= value_diff <= 9:
                small_errors += 1
            else:
                unknown_errors += 1
        except ValueError:
            unknown_errors += 1

        if len(misclassified_samples) <= 10:
            print(f"Query: {query} -> True Result: {true_label} -> Predicted Result: {predicted_label}")

    print("\nLength-based Misclassification Analysis:")
    lengths = sorted(error_by_length.keys())
    error_counts = [error_by_length[length] for length in lengths]
    total_counts = [length_count[length] for length in lengths]

    print("\nTotal counts and misclassification counts by query length (without spaces):")
    for length in lengths:
        total = length_count[length]
        misclassified = error_by_length[length]
        print(f"Length {length}: Total samples = {total}, Misclassified = {misclassified}")

    print("\nError Type-based Misclassification Analysis:")
    print(f"Large Errors (>10 difference): {large_errors}")
    print(f"Small Errors (1-9 difference): {small_errors}")
    print(f"Unknown Errors (non-numeric or 0 difference): {unknown_errors}")

test_size = 0.3
x_train, x_test, y_train, y_test, x_train_text, x_test_text, y_train_text, y_test_text = train_test_split(
    X_text_onehot, y_text_onehot, X_text, y_text, test_size=test_size
)

model = build_text2text_model()
model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1)

analyze_errors_and_length(model, x_test, y_test, x_test_text, y_test_text, unique_characters)

"""
---
---

## II. Image to text RNN Model


### Description**
we used Convolutional Neural Networks(CNN) with TimeDis-
tributed layers to extract features from images.These features
are then pass through an LSTM layer to generate a context
vector that caputures the image’s key information. Then the
decode we used another LSTM layer to generate a sequence
of word, outputting text step by step.The output at each step
is a probability distribution over all possible characters, using
a softmax activation. We choose this model to do experiment
because it performed better in test set and traing set."""

def build_image2text_model():
  image_input = Input(shape=(max_query_length, 28, 28, 1), name="Image_Input")
  x = TimeDistributed(Conv2D(32, (3, 3), activation="relu", padding="same"), name="Conv2D_1")(image_input)
  x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_1")(x)
  x = TimeDistributed(Conv2D(64, (3, 3), activation="relu", padding="same"), name="Conv2D_2")(x)
  x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_2")(x)
  x = TimeDistributed(Flatten(), name="Flatten")(x)
  encoded = LSTM(1024, return_sequences=False, dropout=0.3, name="Encoder_LSTM")(x)
  repeated_context = RepeatVector(max_answer_length, name="Repeat_Vector")(encoded)
  decoded = LSTM(1024, return_sequences=True, dropout=0.3, name="Decoder_LSTM")(repeated_context)
  output = TimeDistributed(Dense(len_unique_characters, activation='softmax'), name="Output_Layer")(decoded)
  model = Model(inputs=image_input, outputs=output, name="Image2Text_Model")
  model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])
  model.summary()
  return model

from sklearn.model_selection import train_test_split
y_text_onehot = encode_labels(y_text, max_len=max_answer_length)
len_unique_characters = len(unique_characters)
x_train, x_test, y_train, y_test = train_test_split(X_img, y_text_onehot, test_size=0.3, random_state=42)
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
model = build_image2text_model()
history = model.fit(x_train, y_train, epochs=20, batch_size=16)
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

"""# This is image-to-text structure

"""

from tensorflow.keras.utils import plot_model
model = build_image2text_model()
plot_model(model, to_file='image2text_model.png', show_shapes=True, show_layer_names=True, dpi=72)

"""#Mistakes types of image-to-text


The error types of outcomes are mainly two kinds when we com-
pare the incorrect results with the actual values. Firstly, errors are
classified into three categories: small-range errors (differences less
than 10), large-range errors (differences greater than 10), and unrec-
ognized errors (cases where there is no result due to low probability
among all classes).
"""

#Two LTSM layers
def build_image2text_model():
    image_input = Input(shape=(max_query_length, 28, 28, 1), name="Image_Input")
    x = TimeDistributed(Conv2D(32, (3, 3), activation="relu", padding="same"), name="Conv2D_1")(image_input)
    x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_1")(x)
    x = TimeDistributed(Conv2D(64, (3, 3), activation="relu", padding="same"), name="Conv2D_2")(x)
    x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_2")(x)
    x = TimeDistributed(Flatten(), name="Flatten")(x)
    encoded_1 = LSTM(1024, return_sequences=True, dropout=0.3, name="Encoder_LSTM_1")(x)
    encoded_2 = LSTM(1024, return_sequences=False, dropout=0.3, name="Encoder_LSTM_2")(encoded_1)
    repeated_context = RepeatVector(max_answer_length, name="Repeat_Vector")(encoded_2)
    decoded = LSTM(1024, return_sequences=True, dropout=0.3, name="Decoder_LSTM")(repeated_context)
    output = TimeDistributed(Dense(len_unique_characters, activation='softmax'), name="Output_Layer")(decoded)
    model = Model(inputs=image_input, outputs=output, name="Image2Text_Model")
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    model.summary()
    return model
"""
#Three layers
def build_image2text_model():
    image_input = Input(shape=(max_query_length, 28, 28, 1), name="Image_Input")
    x = TimeDistributed(Conv2D(32, (3, 3), activation="relu", padding="same"), name="Conv2D_1")(image_input)
    x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_1")(x)
    x = TimeDistributed(Conv2D(64, (3, 3), activation="relu", padding="same"), name="Conv2D_2")(x)
    x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_2")(x)
    x = TimeDistributed(Flatten(), name="Flatten")(x)

    encoded_1 = LSTM(1024, return_sequences=True, dropout=0.3, name="Encoder_LSTM_1")(x)
    encoded_2 = LSTM(1024, return_sequences=True, dropout=0.3, name="Encoder_LSTM_2")(encoded_1)
    encoded_3 = LSTM(1024, return_sequences=False, dropout=0.3, name="Encoder_LSTM_3")(encoded_2)

    repeated_context = RepeatVector(max_answer_length, name="Repeat_Vector")(encoded_3)
    decoded = LSTM(1024, return_sequences=True, dropout=0.3, name="Decoder_LSTM")(repeated_context)
    output = TimeDistributed(Dense(len_unique_characters, activation='softmax'), name="Output_Layer")(decoded)

    model = Model(inputs=image_input, outputs=output, name="Image2Text_Model")
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

    model.summary()
    return model
"""

import numpy as np
import cv2
def count_non_zero_images(X_image):
  num_digits = 0
  for i in range(X_image.shape[0]):
    sub_image = X_image[i]
    if np.sum(sub_image > 0) > 0:
      num_digits += 1
  return num_digits

y_text_onehot = encode_labels(y_text, max_len=max_answer_length)
len_unique_characters = len(unique_characters)
x_train, x_test, y_train, y_test = train_test_split(X_img, y_text_onehot, test_size=0.3, random_state=42)
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
model = build_image2text_model()
history = model.fit(x_train, y_train, epochs=5, batch_size=32)
predictions = model.predict(x_train)
num_digits_list = [count_non_zero_images(x) for x in x_train]
y_train_digits = np.array(num_digits_list)
for i in range(10):
    print(f"length: {y_train_digits[i]}, True: {decode_labels(y_train[i])}, Predicted: {decode_labels(predictions[i])}")

mismatch_count = 0
error_greater_than_10 = 0
error_between_1_and_9 = 0
unrecognized_errors = 0
for i in range(len(y_train)):
  true_labels = np.argmax(y_train[i], axis=-1)
  predicted_labels = np.argmax(predictions[i], axis=-1)
  if not np.array_equal(true_labels, predicted_labels):
      mismatch_count += 1
  try:
      diff = np.abs(predicted_labels - true_labels)
  except TypeError:
      diff = np.nan
      unrecognized_errors += 1
  if np.any(np.isnan(diff)):
      unrecognized_errors += 1
  elif np.any(diff > 10):
      error_greater_than_10 += 1
  elif np.any((diff >= 1) & (diff <= 9)):
      error_between_1_and_9 += 1
  elif np.all(predicted_labels == 0):
      unrecognized_errors += 1

print(f"Total number of mismatched predictions: {mismatch_count}")
print(f"Total errors with difference > 10: {error_greater_than_10}")
print(f"Total errors with difference between 1 and 9: {error_between_1_and_9}")
print(f"Total unrecognized errors (all zeros or undesired): {unrecognized_errors}")
total_errors = error_greater_than_10 + error_between_1_and_9 + unrecognized_errors

import numpy as np
import cv2
def count_non_zero_images(X_image):
  num_digits = 0
  for i in range(X_image.shape[0]):
    sub_image = X_image[i]
    if np.sum(sub_image > 0) > 0:
      num_digits += 1
  return num_digits

import numpy as np
num_digits_list = [count_non_zero_images(x) for x in x_train]
y_train_digits = np.array(num_digits_list)
lengths_inconsistency = {}
for i in range(len(x_train)):
    true_label = decode_labels(y_train[i])
    predicted_label = decode_labels(predictions[i])
    num_digits = y_train_digits[i]
    if len(true_label) == len(predicted_label):
        if true_label != predicted_label:
            if num_digits not in lengths_inconsistency:
                lengths_inconsistency[num_digits] = 0
            lengths_inconsistency[num_digits] += 1
    else:
        if num_digits not in lengths_inconsistency:
            lengths_inconsistency[num_digits] = 0
        lengths_inconsistency[num_digits] += 1
for length, count in lengths_inconsistency.items():
    print(f"Length {length}: {count} inconsistencies")

"""#Different test size

We tried multiple different splits for your training
and test sets (50% train - 50% test; 25% train - 75% test, 10% train, 90% test etc.) and evaluate the
accuracy and generalization capability of your models.

As a result,On training set , the smallest train-test split ratio of 10%-90%
performs best (as shown in Figure12) as more training data is avail-
able for the model to learn from. Conversely, with smaller training
sets, the model might underfit, leading to higher loss values and
low accuracy.
"""

from sklearn.model_selection import train_test_split
y_text_onehot = encode_labels(y_text, max_len=3)
split_number=[0.1,0.25,0.4,0.5,0.6,0.75,0.9]
all_histories = {}
for i in split_number:
  x_train, x_test, y_train, y_test = train_test_split(X_img, y_text_onehot, test_size=i)
  model = build_image2text_model()
  history = model.fit(x_train, y_train, epochs=20, batch_size=3)
  test_loss, test_accuracy = model.evaluate(x_test, y_test)
  all_histories[i] = {
    'loss': history.history['loss'],
    'accuracy': history.history['accuracy'],
    'test_loss': test_loss,
    'test_accuracy': test_accuracy
}

fig, axes = plt.subplots(1, 2, figsize=(8, 5))
for i, history in all_histories.items():
    axes[0].plot(history['loss'], label=f'Training Loss (test_size={i})')
axes[0].set_title('Training Loss of Different Test Sizes')
axes[0].set_xlabel('Epochs')
axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid()
for i, history in all_histories.items():
    axes[1].plot(history['accuracy'], label=f'Training Accuracy (test_size={i})')
axes[1].set_title('Training Accuracy of Different Test Sizes')
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('Accuracy')
axes[1].legend()
axes[1].grid()
plt.tight_layout()
plt.show()
for split, metrics in all_histories.items():
    print(f"Test size: {split}")
    print(f"Test Loss: {metrics['test_loss']:.4f}, Test Accuracy: {metrics['test_accuracy']:.4f}")
    print("-" * 30)

"""# Comparison the outputs
In this part, we used Three ways to build this model.  It uses LSTM layers to encode a sequence of images into a fixed-size vector and then decodes it into a sequence of characters. The model includes dropout for regularization and uses the softmax activation function in the output layer for multi-class classification. It is compiled with categorical cross-entropy loss and the Adam optimizer for training.


We found this model is overfitting.
"""

#another approach
def build_image2text_model():
  image_input = Input(shape=(max_query_length, 28, 28, 1), name="Image_Input")
  flattened_images = TimeDistributed(Flatten(), name="Flatten_Images")(image_input)
  encoded = LSTM(512, return_sequences=False, dropout=0.3)(flattened_images)
  repeated_context = RepeatVector(max_answer_length)(encoded)
  decoded = LSTM(512, return_sequences=True, dropout=0.3)(repeated_context)
  output = TimeDistributed(Dense(len(unique_characters), activation='softmax'))(decoded)
  model = Model(inputs=image_input, outputs=output)
  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
  return model

from sklearn.model_selection import train_test_split
y_text_onehot = encode_labels(y_text, max_len=max_answer_length)
x_train, x_test, y_train, y_test = train_test_split(X_img, y_text_onehot, test_size=0.3, random_state=42)
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
model = build_image2text_model()
history = model.fit(x_train, y_train, epochs=20, batch_size=16)
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

"""# Add LTSM layers

"""

from sklearn.model_selection import train_test_split
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import LSTM, TimeDistributed, Conv2D, MaxPooling2D, Flatten, RepeatVector, Dense
from tensorflow.keras.optimizers import Adam
def build_image2text_model(num_encoder_layers):
    image_input = Input(shape=(max_query_length, 28, 28, 1), name="Image_Input")
    x = TimeDistributed(Conv2D(32, (3, 3), activation="relu", padding="same"), name="Conv2D_1")(image_input)
    x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_1")(x)
    x = TimeDistributed(Conv2D(64, (3, 3), activation="relu", padding="same"), name="Conv2D_2")(x)
    x = TimeDistributed(MaxPooling2D((2, 2)), name="MaxPooling2D_2")(x)
    x = TimeDistributed(Flatten(), name="Flatten")(x)
    for i in range(num_encoder_layers - 1):
        x = LSTM(1024, return_sequences=True, dropout=0.3, name=f"Encoder_LSTM_{i+1}")(x)
    encoded = LSTM(1024, return_sequences=False, dropout=0.3, name=f"Encoder_LSTM_{num_encoder_layers}")(x)
    repeated_context = RepeatVector(max_answer_length, name="Repeat_Vector")(encoded)
    decoded = LSTM(1024, return_sequences=True, dropout=0.3, name="Decoder_LSTM")(repeated_context)
    output = TimeDistributed(Dense(len_unique_characters, activation='softmax'), name="Output_Layer")(decoded)
    model = Model(inputs=image_input, outputs=output, name="Image2Text_Model")
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    model.summary()
    return model

y_text_onehot = encode_labels(y_text, max_len=max_answer_length)
len_unique_characters = len(unique_characters)
x_train, x_test, y_train, y_test = train_test_split(X_img, y_text_onehot, test_size=0.3, random_state=42)
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
all_histories = {}
num_encoder_layers_list = [1, 2, 3, 4]
for num_encoder_layers in num_encoder_layers_list:
    print(f"\nTraining with {num_encoder_layers} encoder layers")
    model = build_image2text_model(num_encoder_layers=num_encoder_layers)
    history = model.fit(x_train, y_train, epochs=20, batch_size=16)
    test_loss, test_accuracy = model.evaluate(x_test, y_test)
    print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")
    all_histories[num_encoder_layers] = {
        'loss': history.history['loss'],
        'accuracy': history.history['accuracy'],
    }

import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 2, figsize=(9, 5))
for num_layers, history in all_histories.items():
    epochs = range(1, len(history['accuracy']) + 1)
    axs[0].plot(epochs, history['accuracy'], label=f'Train Accuracy: {num_layers} LSTM layers')
axs[0].set_title('Training Accuracy for different LTSM layer numbers')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Accuracy')
axs[0].legend()
axs[0].grid()
for num_layers, history in all_histories.items():
    epochs = range(1, len(history['loss']) + 1)
    axs[1].plot(epochs, history['loss'], label=f'Train Loss: {num_layers} LSTM layers')
axs[1].set_title('Training Loss for different LTSM layer numbers')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Loss')
axs[1].legend()
axs[1].grid()
plt.tight_layout()
plt.show()

"""---
---

## III. Text to image RNN Model
### Description**
 This Text-to-Image model combines LSTM for encoding the input text and Conv2DTranspose layers for progressively upscaling the feature map to generate an image. Firstly, the text is encoded into a vextore by the LTSM layer, then tranformed through a dense into a 7*7*128 tensor.TWo decovolutional layers upsamples the feature map, increasing spatial deimensions to generate the final image. The model uses binary crossentropy as the loss function and use Adam optimizer to produce a visual representation from the input text.


"""

from tensorflow.keras.layers import Input, Conv2DTranspose, Dense, Permute
def build_test2image_model():
    encoder_inputs = Input(shape=(max_query_length, len(unique_characters)), name="Encoder_Input")
    encoder_lstm = LSTM(256, return_sequences=False, name="Encoder_LSTM")
    encoder_output = encoder_lstm(encoder_inputs)
    decoder_dense = Dense(7 * 7 * 128, activation='relu', name="Decoder_Dense")(encoder_output)
    decoder_reshape = Reshape((7, 7, 128), name="Decoder_Reshape")(decoder_dense)
    decoder_deconv1 = Conv2DTranspose(64, kernel_size=3, strides=2, padding="same", activation="relu", name="Deconv1")(decoder_reshape)
    decoder_deconv2 = Conv2DTranspose(32, kernel_size=3, strides=2, padding="same", activation="relu", name="Deconv2")(decoder_deconv1)
    decoder_output = Conv2DTranspose(max_answer_length, kernel_size=3, strides=1, padding="same", activation="sigmoid", name="Deconv_Output")(decoder_deconv2)
    decoder_output = Permute((3, 1, 2), name="Permute_Output")(decoder_output)
    model = Model(encoder_inputs, decoder_output, name="Text_to_Image_Model")
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=['binary_accuracy'])
    return model

from tensorflow.keras.utils import plot_model
model = build_test2image_model()
plot_model(model, to_file='image2text_model.png', show_shapes=True, show_layer_names=True, dpi=72)

"""# Results of text-to-image model
We recorded the loss and binary-accuracy.The fall of
the loss function indicates that the model’s predictions get closer
to the true values.There are many reasons to result in the accu-
racy flutuating like learning rate too high, imbalanced data and
optimaizer configuration issues.
"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_text_onehot, y_img, test_size=0.2)
print(X_text_onehot.shape)
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
model1=build_test2image_model()
history = model1.fit(X_text_onehot, y_img, batch_size=32, epochs=50)
test_loss = model1.evaluate(x_test, y_test)
print(f"Test Loss: {test_loss}")

plt.figure(figsize=(8, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.title('Training Loss per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.subplot(1, 2, 2)
plt.plot(history.history['binary_accuracy'], label='Train Accuracy', color='green')
plt.title('Training Accuarcy per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

"""# output of text-to-image model

 The
images are generated with model using LSTMs to processes data
one step at a time, leading to accumulated errors, distortion, and
loss of details in the output, ultimately resulting in low quality of
images.

"""

def visualize_prediction(idx):
    query = X_text[idx]
    true_images = y_img[idx]
    predicted_images = model1.predict(np.expand_dims(X_text_onehot[idx], axis=0))[0]
    print(f"Predicted images shape: {predicted_images.shape}")
    print(y_img.shape)
    print(f"Pixel value range: {np.min(predicted_images)}, {np.max(predicted_images)}")
    true_combined = np.hstack([true_images[i].reshape(28, 28) for i in range(max_answer_length)])
    predicted_combined = np.hstack([predicted_images[i].reshape(28, 28) for i in range(max_answer_length)])
    print(f"Query: {query}")
    plt.figure(figsize=(5, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(true_combined, cmap='gray')
    plt.title("True Images", fontsize=14)
    plt.axis('off')
    plt.subplot(1, 2, 2)
    plt.imshow(predicted_combined, cmap='gray')
    plt.title("Predicted Images", fontsize=14)
    plt.axis('off')
    plt.suptitle(f"Query: {query}", fontsize=16, y=0.7)
    plt.tight_layout()
    plt.show()
for _ in range(5):
    idx = np.random.randint(0, len(x_test))
    visualize_prediction(idx)

"""# A separate supervised model
We designed a supervised model using CNNs to evaluate the
generated images of the Text-to-Image model. However, due to the
low quality of the generated images, the CNN failed to accurately
recognize the digits in the images. Therefore, it is inefficient in
evaluating the Text-to-Image model.

Firstly, we can X-text to calculate the answer.It starts by flattening the input and then adds two fully connected (dense) layers with ReLU activation. The final layer has a single neuron to output a continuous value. The model is compiled with the Adam optimizer, mean squared error (MSE) loss function, and mean absolute error (MAE) as a metric.
"""

def calculate_addition_result(text_queries):
    results = []
    for query in text_queries:
        query = query.strip()
        if '+' in query:
            num1, num2 = query.split('+')
            operator = '+'
        elif '-' in query:
            num1, num2 = query.split('-')
            operator = '-'
        else:
            raise ValueError(f"Unrecognized format: {query}")
        num1 = int(num1.strip())
        num2 = int(num2.strip())
        if operator == '+':
            result = num1 + num2
        elif operator == '-':
            result = num1 - num2
        results.append(result)
    return results

def decode_labels(labels):
    predicted = []
    for label in labels:
        pred = np.argmax(label, axis=1)
        predicted_str = ''.join([unique_characters[i] for i in pred])
        predicted.append(predicted_str)
    return predicted
X=decode_labels(x_train)
print(X[:10])
y_num = calculate_addition_result(decode_labels(x_train))
print(y_num)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
def build_regression_model(input_shape):
    model = Sequential()
    model.add(Flatten(input_shape=input_shape))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
    return model
X = model1.predict(x_train)
print(X.shape)

X = np.transpose(X, (0, 2, 3, 1))
print(X.shape)
input_shape = (28, 28, 3)
model = build_regression_model(input_shape)
history = model.fit(X, np.array(y_num), batch_size=32, epochs=10)
predicted_values = model.predict(X)
predicted_values = np.round(predicted_values).astype(int)
y_num = np.array(y_num)
correct_predictions = (predicted_values.flatten() == y_num.flatten())
accuracy = np.mean(correct_predictions)
print(f"Prediction accuracy: {accuracy * 100:.2f}%")
for i in range(10):
    print(f"Input: {x_train[i]} - Predicted: {predicted_values[i][0]} - Actual: {y_num[i]}")

"""# different encoder layer
 Like other models, we also used different layers to measure this model's performance.
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Reshape, Conv2DTranspose, Permute
from tensorflow.keras.optimizers import Adam
def build_text2image_model(num_encoder_layers):
    encoder_inputs = Input(shape=(max_query_length, len(unique_characters)), name="Encoder_Input")
    x = encoder_inputs
    for i in range(num_encoder_layers - 1):
        x = LSTM(256, return_sequences=True, name=f"Encoder_LSTM_{i+1}")(x)
    encoder_output = LSTM(256, return_sequences=False, name=f"Encoder_LSTM_{num_encoder_layers}")(x)
    decoder_dense = Dense(7 * 7 * 128, activation='relu', name="Decoder_Dense")(encoder_output)
    decoder_reshape = Reshape((7, 7, 128), name="Decoder_Reshape")(decoder_dense)
    decoder_deconv1 = Conv2DTranspose(64, kernel_size=3, strides=2, padding="same", activation="relu", name="Deconv1")(decoder_reshape)
    decoder_deconv2 = Conv2DTranspose(32, kernel_size=3, strides=2, padding="same", activation="relu", name="Deconv2")(decoder_deconv1)
    decoder_output = Conv2DTranspose(max_answer_length, kernel_size=3, strides=1, padding="same", activation="sigmoid", name="Deconv_Output")(decoder_deconv2)
    decoder_output = Permute((3, 1, 2), name="Permute_Output")(decoder_output)
    model = Model(encoder_inputs, decoder_output, name="Text_to_Image_Model")
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['binary_accuracy'])
    model.summary()
    return model

from sklearn.model_selection import train_test_split
num_encoder_layers_list = [1, 2, 3, 4]
all_histories = {}
X_text_onehot = encode_labels(X_text, max_len=5)
y_text_onehot = encode_labels(y_text, max_len=max_answer_length)
x_train, x_test, y_train, y_test = train_test_split(X_text_onehot, y_img, test_size=0.3, random_state=42)
for num_encoder_layers in num_encoder_layers_list:
    print(f"\nTraining with {num_encoder_layers} encoder LSTM layers")
    model = build_text2image_model(num_encoder_layers=num_encoder_layers)
    history = model.fit(
        x_train,
        y_train,
        epochs=20,
        batch_size=16,
    )

    test_loss, test_binary_accuracy = model.evaluate(x_test, y_test)
    print(f"Test Loss: {test_loss}, Test Binary Accuracy: {test_binary_accuracy}")
    all_histories[num_encoder_layers] = {
        'loss': history.history['loss'],
        'binary_accuracy': history.history['binary_accuracy'],
    }

import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 2, figsize=(8, 5))
for num_layers, history in all_histories.items():
    epochs = range(1, len(history['loss']) + 1)
    axs[0].plot(epochs, history['loss'], label=f'Train Loss: {num_layers} LSTM layers')
axs[0].set_title('Loss per Epoch')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()
axs[0].grid()
for num_layers, history in all_histories.items():
    epochs = range(1, len(history['binary_accuracy']) + 1)
    axs[1].plot(epochs, history['binary_accuracy'], label=f'Train Binary Accuracy: {num_layers} LSTM layers')
axs[1].set_title('Binary Accuracy per Epoch')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()
axs[1].grid()
plt.tight_layout()
plt.show()